{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T06:36:57.238378Z","iopub.execute_input":"2025-06-24T06:36:57.238597Z","iopub.status.idle":"2025-06-24T06:36:59.355108Z","shell.execute_reply.started":"2025-06-24T06:36:57.238578Z","shell.execute_reply":"2025-06-24T06:36:59.353956Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\n/kaggle/input/drw-crypto-market-prediction/train.parquet\n/kaggle/input/drw-crypto-market-prediction/test.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nDRW Crypto Market Prediction - Dual Model Approach\nCombines original model with feature-augmented model for ensemble prediction\n\"\"\"\n\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom xgboost import XGBRegressor\nfrom scipy.stats import pearsonr\nfrom scipy.special import expit\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\n\nprint(\"=\" * 80)\nprint(\"DRW CRYPTO MARKET PREDICTION - DUAL MODEL APPROACH\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# PART 1: TARGETED FEATURE ENGINEERING\n# ==============================================================================\n\nclass TargetedFeatureEngineer:\n    \"\"\"\n    Create specific complex interaction features for augmented model.\n    \"\"\"\n    \n    def __init__(self):\n        self.feature_scaler = RobustScaler()\n    \n    def create_augmented_features(self, df, base_features):\n        \"\"\"\n        Create at least 5 complex interaction features based on top performing features.\n        \"\"\"\n        print(\"\\nCreating targeted augmented features...\")\n        \n        augmented_features = []\n        feature_names = []\n        \n        # 1. COMPLEX INTERACTIONS BETWEEN TOP FEATURES\n        # Based on importance: X862, X852, X345, X532, X888\n        if all(f in df.columns for f in ['X862', 'X852', 'X345']):\n            # Three-way interaction with non-linearity\n            feat1 = df['X862'].values\n            feat2 = df['X852'].values\n            feat3 = df['X345'].values\n            \n            # Feature 1: Non-linear three-way interaction\n            interaction = np.tanh(feat1) * np.exp(-np.abs(feat2) / 2) * np.sign(feat3)\n            augmented_features.append(interaction)\n            feature_names.append('complex_interaction_862_852_345')\n            \n            # Feature 2: Polynomial ratio\n            ratio_poly = (feat1 ** 2) / (feat2 ** 2 + 1)\n            augmented_features.append(ratio_poly)\n            feature_names.append('poly_ratio_862_852')\n            \n            # NEW: Subtraction interactions\n            sub_interaction1 = feat1 - feat2\n            augmented_features.append(sub_interaction1)\n            feature_names.append('X862_minus_X852')\n            \n            sub_interaction2 = feat2 - feat3\n            augmented_features.append(sub_interaction2)\n            feature_names.append('X852_minus_X345')\n            \n            # NEW: RBF transformations\n            # RBF between X862 and X852\n            rbf_sigma = np.std(feat1 - feat2) + 1e-6\n            rbf_862_852 = np.exp(-((feat1 - feat2) ** 2) / (2 * rbf_sigma ** 2))\n            augmented_features.append(rbf_862_852)\n            feature_names.append('rbf_862_852')\n        \n        # 2. MARKET MICROSTRUCTURE COMPLEX FEATURES\n        if all(f in df.columns for f in ['bid_qty', 'ask_qty', 'volume', 'buy_qty', 'sell_qty']):\n            bid = df['bid_qty'].values\n            ask = df['ask_qty'].values\n            vol = df['volume'].values\n            buy = df['buy_qty'].values\n            sell = df['sell_qty'].values\n            \n            # Feature 3: Kyle's lambda approximation with non-linearity\n            order_imbalance = (bid - ask) / (bid + ask + 1e-6)\n            flow_imbalance = (buy - sell) / (buy + sell + 1e-6)\n            kyle_lambda = flow_imbalance * np.sqrt(np.abs(order_imbalance)) / (np.log1p(vol) + 1e-6)\n            augmented_features.append(kyle_lambda)\n            feature_names.append('kyle_lambda_complex')\n            \n            # Feature 4: Volatility-adjusted pressure\n            total_pressure = bid + ask\n            vol_adj_pressure = np.log1p(total_pressure) * np.exp(-vol / (vol.mean() + 1e-6))\n            augmented_features.append(vol_adj_pressure)\n            feature_names.append('vol_adjusted_pressure')\n            \n            # Feature 5: Trade intensity asymmetry\n            buy_intensity = buy / (vol + 1e-6)\n            sell_intensity = sell / (vol + 1e-6)\n            intensity_asymmetry = np.sign(buy_intensity - sell_intensity) * \\\n                                 np.log1p(np.abs(buy_intensity - sell_intensity))\n            augmented_features.append(intensity_asymmetry)\n            feature_names.append('trade_intensity_asymmetry')\n            \n            # NEW: Bid-Ask subtraction and kernel\n            bid_ask_diff = bid - ask\n            augmented_features.append(bid_ask_diff)\n            feature_names.append('bid_minus_ask')\n            \n            # NEW: Gaussian kernel on volume\n            vol_kernel = np.exp(-((vol - vol.mean()) ** 2) / (2 * (vol.std() + 1e-6) ** 2))\n            augmented_features.append(vol_kernel)\n            feature_names.append('volume_gaussian_kernel')\n        \n        # 3. CROSS-DOMAIN INTERACTIONS\n        if all(f in df.columns for f in ['X532', 'X888', 'volume']):\n            # Feature 6: Technical indicator with volume\n            technical = df['X532'].values * df['X888'].values\n            vol = df['volume'].values\n            vol_weighted_tech = technical * np.log1p(vol) / (vol.std() + 1e-6)\n            augmented_features.append(vol_weighted_tech)\n            feature_names.append('volume_weighted_technical')\n            \n            # NEW: X532 + X888 combination\n            tech_sum = df['X532'].values + df['X888'].values\n            augmented_features.append(tech_sum)\n            feature_names.append('X532_plus_X888')\n            \n            # NEW: RBF between X532 and X888\n            tech_diff = df['X532'].values - df['X888'].values\n            tech_rbf = np.exp(-tech_diff ** 2 / (2 * (np.std(tech_diff) + 1e-6) ** 2))\n            augmented_features.append(tech_rbf)\n            feature_names.append('rbf_532_888')\n        \n        # 4. REGIME-BASED FEATURES\n        if 'X137' in df.columns and 'X302' in df.columns:\n            # Feature 7: Regime detection feature\n            feat1 = df['X137'].values\n            feat2 = df['X302'].values\n            \n            # Create regime boundaries\n            regime_indicator = np.where(\n                (feat1 > np.percentile(feat1, 75)) & (feat2 > np.percentile(feat2, 75)), 1,\n                np.where(\n                    (feat1 < np.percentile(feat1, 25)) & (feat2 < np.percentile(feat2, 25)), -1,\n                    0\n                )\n            )\n            augmented_features.append(regime_indicator)\n            feature_names.append('regime_indicator_137_302')\n            \n            # NEW: Laplacian kernel for regime\n            laplacian_kernel = np.exp(-np.abs(feat1 - feat2) / (np.std(feat1 - feat2) + 1e-6))\n            augmented_features.append(laplacian_kernel)\n            feature_names.append('laplacian_kernel_137_302')\n        \n        # 5. DISTANCE-BASED CLUSTER FEATURES\n        if all(f in df.columns for f in ['X178', 'X168', 'X612']):\n            # Feature 8: Multi-dimensional distance from center\n            feat1 = (df['X178'].values - df['X178'].mean()) / (df['X178'].std() + 1e-6)\n            feat2 = (df['X168'].values - df['X168'].mean()) / (df['X168'].std() + 1e-6)\n            feat3 = (df['X612'].values - df['X612'].mean()) / (df['X612'].std() + 1e-6)\n            \n            distance = np.sqrt(feat1**2 + feat2**2 + feat3**2)\n            distance_kernel = np.exp(-distance**2 / 2)\n            augmented_features.append(distance_kernel)\n            feature_names.append('multidim_distance_kernel')\n            \n            # NEW: Pairwise differences\n            diff_178_168 = df['X178'].values - df['X168'].values\n            augmented_features.append(diff_178_168)\n            feature_names.append('X178_minus_X168')\n            \n            diff_168_612 = df['X168'].values - df['X612'].values\n            augmented_features.append(diff_168_612)\n            feature_names.append('X168_minus_X612')\n        \n        # 6. NEW: Additional top feature interactions\n        if 'X598' in df.columns and 'X862' in df.columns:\n            # X598 is often important\n            f598 = df['X598'].values\n            f862 = df['X862'].values\n            \n            # Subtraction\n            diff_598_862 = f598 - f862\n            augmented_features.append(diff_598_862)\n            feature_names.append('X598_minus_X862')\n            \n            # RBF\n            rbf_598_862 = np.exp(-((f598 - f862) ** 2) / (2 * (np.std(f598 - f862) + 1e-6) ** 2))\n            augmented_features.append(rbf_598_862)\n            feature_names.append('rbf_598_862')\n        \n        # Stack all features\n        if augmented_features:\n            augmented_array = np.column_stack(augmented_features)\n            print(f\"  Created {len(feature_names)} augmented features:\")\n            for name in feature_names:\n                print(f\"    - {name}\")\n            return augmented_array, feature_names\n        else:\n            return np.zeros((len(df), 0)), []\n\n# ==============================================================================\n# PART 2: DUAL MODEL PREDICTOR\n# ==============================================================================\n\nclass DualModelCryptoPredictor:\n    \"\"\"Train both original and augmented models, then ensemble predictions.\"\"\"\n    \n    def __init__(self):\n        # Paths\n        self.TRAIN_PATH = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n        self.TEST_PATH = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n        self.SUBMISSION_PATH = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n        \n        # Original features (unchanged)\n        self.ORIGINAL_FEATURES = [\n            \"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n            \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n            \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\", \"X888\", \"X421\", \"X333\"\n        ]\n        \n        self.LABEL_COLUMN = \"label\"\n        self.N_FOLDS = 3\n        self.RANDOM_STATE = 42\n        \n        # XGBoost parameters (unchanged)\n        self.XGB_PARAMS = {\n            \"tree_method\": \"hist\",\n            \"device\": \"cpu\",\n            \"colsample_bylevel\": 0.4778,\n            \"colsample_bynode\": 0.3628,\n            \"colsample_bytree\": 0.7107,\n            \"gamma\": 1.7095,\n            \"learning_rate\": 0.02213,\n            \"max_depth\": 20,\n            \"max_leaves\": 12,\n            \"min_child_weight\": 16,\n            \"n_estimators\": 1667,\n            \"subsample\": 0.06567,\n            \"reg_alpha\": 39.3524,\n            \"reg_lambda\": 75.4484,\n            \"verbosity\": 0,\n            \"random_state\": self.RANDOM_STATE,\n            \"n_jobs\": -1,\n            \"verbose\": False,\n        }\n        \n        self.MODEL_SLICES = [\n            {\"name\": \"full_data\", \"cutoff\": 0},\n            {\"name\": \"last_75pct\", \"cutoff\": 0},  \n            {\"name\": \"last_50pct\", \"cutoff\": 0}\n        ]\n        \n        self.feature_engineer = TargetedFeatureEngineer()\n        self.scaler = RobustScaler()\n        \n    def create_time_decay_weights(self, n, decay=0.95):\n        \"\"\"Create time decay weights for samples.\"\"\"\n        positions = np.arange(n)\n        normalized = positions / float(n - 1)\n        weights = decay ** (1.0 - normalized)\n        return weights * n / weights.sum()\n    \n    def load_data(self):\n        \"\"\"Load train, test, and submission data.\"\"\"\n        print(\"\\nLoading data...\")\n        train_df = pd.read_parquet(self.TRAIN_PATH).reset_index(drop=True)\n        test_df = pd.read_parquet(self.TEST_PATH).reset_index(drop=True)\n        submission_df = pd.read_csv(self.SUBMISSION_PATH)\n        \n        print(f\"Loaded train: {train_df.shape}, test: {test_df.shape}\")\n        return train_df, test_df, submission_df\n    \n    def train_and_predict(self, train_df, test_df, features, model_name=\"Model\"):\n        \"\"\"Train XGBoost model with cross-validation and make predictions.\"\"\"\n        print(f\"\\nTraining {model_name}...\")\n        \n        n_samples = len(train_df)\n        \n        # Set slice cutoffs\n        self.MODEL_SLICES[1][\"cutoff\"] = int(0.25 * n_samples)\n        self.MODEL_SLICES[2][\"cutoff\"] = int(0.50 * n_samples)\n        \n        # Prepare storage\n        oof_preds = {sl[\"name\"]: np.zeros(n_samples) for sl in self.MODEL_SLICES}\n        test_preds = {sl[\"name\"]: np.zeros(len(test_df)) for sl in self.MODEL_SLICES}\n        \n        full_weights = self.create_time_decay_weights(n_samples)\n        kf = KFold(n_splits=self.N_FOLDS, shuffle=False)\n        \n        # Cross-validation\n        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df), start=1):\n            print(f\"\\n--- {model_name} Fold {fold}/{self.N_FOLDS} ---\")\n            X_valid = train_df.iloc[valid_idx][features]\n            y_valid = train_df.iloc[valid_idx][self.LABEL_COLUMN]\n            \n            for sl in self.MODEL_SLICES:\n                slice_name = sl[\"name\"]\n                cutoff = sl[\"cutoff\"]\n                subset = train_df.iloc[cutoff:].reset_index(drop=True)\n                rel_idx = train_idx[train_idx >= cutoff] - cutoff\n                \n                print(f\"Training {slice_name}...\")\n                X_train = subset.iloc[rel_idx][features]\n                y_train = subset.iloc[rel_idx][self.LABEL_COLUMN]\n                \n                # Sample weights\n                if cutoff == 0:\n                    sw = full_weights[train_idx]\n                else:\n                    sw_total = self.create_time_decay_weights(len(subset))\n                    sw = sw_total[rel_idx]\n                \n                # Train model\n                model = XGBRegressor(**self.XGB_PARAMS)\n                model.fit(\n                    X_train, y_train, \n                    sample_weight=sw,\n                    eval_set=[(X_valid, y_valid)],\n                    verbose=100\n                )\n                \n                # OOF predictions\n                mask = valid_idx >= cutoff\n                if mask.any():\n                    idxs = valid_idx[mask]\n                    oof_preds[slice_name][idxs] = model.predict(train_df.iloc[idxs][features])\n                if cutoff > 0 and (~mask).any():\n                    oof_preds[slice_name][valid_idx[~mask]] = oof_preds[\"full_data\"][valid_idx[~mask]]\n                \n                # Test predictions\n                test_preds[slice_name] += model.predict(test_df[features])\n        \n        # Average test predictions\n        for slice_name in test_preds:\n            test_preds[slice_name] /= self.N_FOLDS\n        \n        # Compute Pearson scores\n        pearson_scores = {\n            slice_name: pearsonr(train_df[self.LABEL_COLUMN], preds)[0]\n            for slice_name, preds in oof_preds.items()\n        }\n        \n        print(f\"\\n{model_name} Pearson scores by slice:\")\n        for slice_name, score in pearson_scores.items():\n            print(f\"  {slice_name}: {score:.4f}\")\n        \n        # Simple ensemble\n        oof_ensemble = np.mean(list(oof_preds.values()), axis=0)\n        test_ensemble = np.mean(list(test_preds.values()), axis=0)\n        ensemble_score = pearsonr(train_df[self.LABEL_COLUMN], oof_ensemble)[0]\n        \n        print(f\"\\n{model_name} Ensemble Pearson score: {ensemble_score:.4f}\")\n        \n        return test_ensemble, ensemble_score\n    \n    def prepare_augmented_data(self, train_df, test_df):\n        \"\"\"Prepare data with augmented features.\"\"\"\n        print(\"\\nPreparing augmented features...\")\n        \n        # Create augmented features\n        train_aug, aug_names = self.feature_engineer.create_augmented_features(\n            train_df, self.ORIGINAL_FEATURES\n        )\n        test_aug, _ = self.feature_engineer.create_augmented_features(\n            test_df, self.ORIGINAL_FEATURES\n        )\n        \n        # Combine with original features\n        train_combined = np.hstack([train_df[self.ORIGINAL_FEATURES].values, train_aug])\n        test_combined = np.hstack([test_df[self.ORIGINAL_FEATURES].values, test_aug])\n        \n        # Scale all features\n        train_scaled = self.scaler.fit_transform(train_combined)\n        test_scaled = self.scaler.transform(test_combined)\n        \n        # Create DataFrames\n        all_features = self.ORIGINAL_FEATURES + aug_names\n        train_final = pd.DataFrame(train_scaled, columns=all_features)\n        train_final['label'] = train_df['label'].values\n        \n        test_final = pd.DataFrame(test_scaled, columns=all_features)\n        \n        return train_final, test_final, all_features\n\n# ==============================================================================\n# PART 3: MAIN EXECUTION\n# ==============================================================================\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    \n    # Initialize predictor\n    predictor = DualModelCryptoPredictor()\n    \n    # Load data\n    train_df, test_df, submission_df = predictor.load_data()\n    \n    # =========================================================================\n    # MODEL 1: ORIGINAL MODEL (UNCHANGED)\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 80)\n    print(\"MODEL 1: ORIGINAL MODEL\")\n    print(\"=\" * 80)\n    \n    # Prepare original data\n    train_original = train_df[predictor.ORIGINAL_FEATURES + ['label']].copy()\n    test_original = test_df[predictor.ORIGINAL_FEATURES].copy()\n    \n    # Train original model\n    original_predictions, original_score = predictor.train_and_predict(\n        train_original, test_original, predictor.ORIGINAL_FEATURES, \"Original Model\"\n    )\n    \n    # Save original predictions (as requested)\n    submission_original = submission_df.copy()\n    submission_original[\"prediction\"] = original_predictions\n    submission_original.to_csv(\"submission.csv\", index=False)\n    print(\"\\nSaved original submission.csv\")\n    \n    # =========================================================================\n    # MODEL 2: AUGMENTED MODEL\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 80)\n    print(\"MODEL 2: AUGMENTED MODEL\")\n    print(\"=\" * 80)\n    \n    # Prepare augmented data\n    train_augmented, test_augmented, augmented_features = predictor.prepare_augmented_data(\n        train_df, test_df\n    )\n    \n    print(f\"\\nTotal features in augmented model: {len(augmented_features)}\")\n    print(f\"New features added: {len(augmented_features) - len(predictor.ORIGINAL_FEATURES)}\")\n    \n    # Train augmented model\n    augmented_predictions, augmented_score = predictor.train_and_predict(\n        train_augmented, test_augmented, augmented_features, \"Augmented Model\"\n    )\n    \n    # =========================================================================\n    # ENSEMBLE: AVERAGE BOTH MODELS\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL ENSEMBLE\")\n    print(\"=\" * 80)\n    \n    # Average predictions\n    ensemble_predictions = (original_predictions + augmented_predictions) / 2\n    \n    # Save ensemble predictions\n    submission_ensemble = submission_df.copy()\n    submission_ensemble[\"prediction\"] = ensemble_predictions\n    submission_ensemble.to_csv(\"submission_ensemble.csv\", index=False)\n    print(\"\\nSaved ensemble submission_ensemble.csv\")\n    \n    # =========================================================================\n    # RESULTS SUMMARY\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"Original Model:\")\n    print(f\"  Features: {len(predictor.ORIGINAL_FEATURES)}\")\n    print(f\"  Score: {original_score:.4f}\")\n    print(f\"\\nAugmented Model:\")\n    print(f\"  Features: {len(augmented_features)}\")\n    print(f\"  Score: {augmented_score:.4f}\")\n    print(f\"\\nExpected Ensemble Performance:\")\n    print(f\"  Better than: {min(original_score, augmented_score):.4f}\")\n    print(f\"  Potential: ~{(original_score + augmented_score) / 2:.4f} to {max(original_score, augmented_score) * 1.02:.4f}\")\n    \n    # Feature importance for augmented model\n    print(\"\\n\" + \"=\" * 80)\n    print(\"AUGMENTED FEATURES IMPORTANCE CHECK\")\n    print(\"=\" * 80)\n    \n    # Quick importance check on augmented features\n    sample_model = XGBRegressor(n_estimators=100, max_depth=6, random_state=42)\n    sample_size = min(50000, len(train_augmented))\n    sample_idx = np.random.choice(len(train_augmented), sample_size, replace=False)\n    \n    sample_model.fit(\n        train_augmented.iloc[sample_idx][augmented_features],\n        train_augmented.iloc[sample_idx]['label']\n    )\n    \n    importance_df = pd.DataFrame({\n        'feature': augmented_features,\n        'importance': sample_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    # Show importance of new features\n    print(\"\\nImportance of new augmented features:\")\n    for feat in augmented_features:\n        if feat not in predictor.ORIGINAL_FEATURES:\n            imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n            rank = list(importance_df['feature']).index(feat) + 1\n            print(f\"  {feat}: Rank {rank}/{len(augmented_features)}, Importance: {imp:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T06:36:59.356920Z","iopub.execute_input":"2025-06-24T06:36:59.357337Z","iopub.status.idle":"2025-06-24T07:14:10.809309Z","shell.execute_reply.started":"2025-06-24T06:36:59.357316Z","shell.execute_reply":"2025-06-24T07:14:10.807333Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDRW CRYPTO MARKET PREDICTION - DUAL MODEL APPROACH\n================================================================================\n\nLoading data...\nLoaded train: (525887, 896), test: (538150, 896)\n\n================================================================================\nMODEL 1: ORIGINAL MODEL\n================================================================================\n\nTraining Original Model...\n\n--- Original Model Fold 1/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.00216\n[100]\tvalidation_0-rmse:0.99508\n[200]\tvalidation_0-rmse:0.99333\n[300]\tvalidation_0-rmse:0.99217\n[400]\tvalidation_0-rmse:0.99180\n[500]\tvalidation_0-rmse:0.99255\n[600]\tvalidation_0-rmse:0.99313\n[700]\tvalidation_0-rmse:0.99403\n[800]\tvalidation_0-rmse:0.99549\n[900]\tvalidation_0-rmse:0.99715\n[1000]\tvalidation_0-rmse:0.99819\n[1100]\tvalidation_0-rmse:0.99981\n[1200]\tvalidation_0-rmse:1.00125\n[1300]\tvalidation_0-rmse:1.00249\n[1400]\tvalidation_0-rmse:1.00324\n[1500]\tvalidation_0-rmse:1.00515\n[1600]\tvalidation_0-rmse:1.00674\n[1666]\tvalidation_0-rmse:1.00782\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.00213\n[100]\tvalidation_0-rmse:0.99441\n[200]\tvalidation_0-rmse:0.99181\n[300]\tvalidation_0-rmse:0.99120\n[400]\tvalidation_0-rmse:0.99077\n[500]\tvalidation_0-rmse:0.99168\n[600]\tvalidation_0-rmse:0.99206\n[700]\tvalidation_0-rmse:0.99341\n[800]\tvalidation_0-rmse:0.99439\n[900]\tvalidation_0-rmse:0.99545\n[1000]\tvalidation_0-rmse:0.99749\n[1100]\tvalidation_0-rmse:0.99899\n[1200]\tvalidation_0-rmse:1.00012\n[1300]\tvalidation_0-rmse:1.00135\n[1400]\tvalidation_0-rmse:1.00280\n[1500]\tvalidation_0-rmse:1.00415\n[1600]\tvalidation_0-rmse:1.00612\n[1666]\tvalidation_0-rmse:1.00704\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.00311\n[100]\tvalidation_0-rmse:0.99620\n[200]\tvalidation_0-rmse:0.99269\n[300]\tvalidation_0-rmse:0.99097\n[400]\tvalidation_0-rmse:0.99155\n[500]\tvalidation_0-rmse:0.99177\n[600]\tvalidation_0-rmse:0.99216\n[700]\tvalidation_0-rmse:0.99360\n[800]\tvalidation_0-rmse:0.99483\n[900]\tvalidation_0-rmse:0.99528\n[1000]\tvalidation_0-rmse:0.99642\n[1100]\tvalidation_0-rmse:0.99786\n[1200]\tvalidation_0-rmse:0.99894\n[1300]\tvalidation_0-rmse:1.00024\n[1400]\tvalidation_0-rmse:1.00133\n[1500]\tvalidation_0-rmse:1.00186\n[1600]\tvalidation_0-rmse:1.00307\n[1666]\tvalidation_0-rmse:1.00424\n\n--- Original Model Fold 2/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.00747\n[100]\tvalidation_0-rmse:1.00330\n[200]\tvalidation_0-rmse:1.00213\n[300]\tvalidation_0-rmse:1.00241\n[400]\tvalidation_0-rmse:1.00297\n[500]\tvalidation_0-rmse:1.00381\n[600]\tvalidation_0-rmse:1.00485\n[700]\tvalidation_0-rmse:1.00578\n[800]\tvalidation_0-rmse:1.00656\n[900]\tvalidation_0-rmse:1.00751\n[1000]\tvalidation_0-rmse:1.00872\n[1100]\tvalidation_0-rmse:1.00989\n[1200]\tvalidation_0-rmse:1.01101\n[1300]\tvalidation_0-rmse:1.01189\n[1400]\tvalidation_0-rmse:1.01240\n[1500]\tvalidation_0-rmse:1.01264\n[1600]\tvalidation_0-rmse:1.01427\n[1666]\tvalidation_0-rmse:1.01445\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.00838\n[100]\tvalidation_0-rmse:1.00585\n[200]\tvalidation_0-rmse:1.00722\n[300]\tvalidation_0-rmse:1.00897\n[400]\tvalidation_0-rmse:1.01049\n[500]\tvalidation_0-rmse:1.01234\n[600]\tvalidation_0-rmse:1.01513\n[700]\tvalidation_0-rmse:1.01757\n[800]\tvalidation_0-rmse:1.01926\n[900]\tvalidation_0-rmse:1.02149\n[1000]\tvalidation_0-rmse:1.02347\n[1100]\tvalidation_0-rmse:1.02587\n[1200]\tvalidation_0-rmse:1.02740\n[1300]\tvalidation_0-rmse:1.02950\n[1400]\tvalidation_0-rmse:1.03102\n[1500]\tvalidation_0-rmse:1.03338\n[1600]\tvalidation_0-rmse:1.03512\n[1666]\tvalidation_0-rmse:1.03637\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.00867\n[100]\tvalidation_0-rmse:1.00558\n[200]\tvalidation_0-rmse:1.00505\n[300]\tvalidation_0-rmse:1.00547\n[400]\tvalidation_0-rmse:1.00721\n[500]\tvalidation_0-rmse:1.00897\n[600]\tvalidation_0-rmse:1.01143\n[700]\tvalidation_0-rmse:1.01315\n[800]\tvalidation_0-rmse:1.01511\n[900]\tvalidation_0-rmse:1.01744\n[1000]\tvalidation_0-rmse:1.02041\n[1100]\tvalidation_0-rmse:1.02258\n[1200]\tvalidation_0-rmse:1.02374\n[1300]\tvalidation_0-rmse:1.02512\n[1400]\tvalidation_0-rmse:1.02699\n[1500]\tvalidation_0-rmse:1.02798\n[1600]\tvalidation_0-rmse:1.02930\n[1666]\tvalidation_0-rmse:1.03036\n\n--- Original Model Fold 3/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.02083\n[100]\tvalidation_0-rmse:1.01795\n[200]\tvalidation_0-rmse:1.01996\n[300]\tvalidation_0-rmse:1.02372\n[400]\tvalidation_0-rmse:1.02745\n[500]\tvalidation_0-rmse:1.03158\n[600]\tvalidation_0-rmse:1.03543\n[700]\tvalidation_0-rmse:1.03941\n[800]\tvalidation_0-rmse:1.04177\n[900]\tvalidation_0-rmse:1.04765\n[1000]\tvalidation_0-rmse:1.04973\n[1100]\tvalidation_0-rmse:1.05090\n[1200]\tvalidation_0-rmse:1.05246\n[1300]\tvalidation_0-rmse:1.05459\n[1400]\tvalidation_0-rmse:1.05792\n[1500]\tvalidation_0-rmse:1.06125\n[1600]\tvalidation_0-rmse:1.06363\n[1666]\tvalidation_0-rmse:1.06459\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.02076\n[100]\tvalidation_0-rmse:1.01736\n[200]\tvalidation_0-rmse:1.01974\n[300]\tvalidation_0-rmse:1.02271\n[400]\tvalidation_0-rmse:1.02752\n[500]\tvalidation_0-rmse:1.02943\n[600]\tvalidation_0-rmse:1.03158\n[700]\tvalidation_0-rmse:1.03840\n[800]\tvalidation_0-rmse:1.04151\n[900]\tvalidation_0-rmse:1.04518\n[1000]\tvalidation_0-rmse:1.04950\n[1100]\tvalidation_0-rmse:1.05132\n[1200]\tvalidation_0-rmse:1.05390\n[1300]\tvalidation_0-rmse:1.05503\n[1400]\tvalidation_0-rmse:1.05512\n[1500]\tvalidation_0-rmse:1.05667\n[1600]\tvalidation_0-rmse:1.05751\n[1666]\tvalidation_0-rmse:1.05833\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.01950\n[100]\tvalidation_0-rmse:1.01798\n[200]\tvalidation_0-rmse:1.01873\n[300]\tvalidation_0-rmse:1.01929\n[400]\tvalidation_0-rmse:1.02256\n[500]\tvalidation_0-rmse:1.02470\n[600]\tvalidation_0-rmse:1.02770\n[700]\tvalidation_0-rmse:1.02973\n[800]\tvalidation_0-rmse:1.03167\n[900]\tvalidation_0-rmse:1.03376\n[1000]\tvalidation_0-rmse:1.03536\n[1100]\tvalidation_0-rmse:1.03654\n[1200]\tvalidation_0-rmse:1.03792\n[1300]\tvalidation_0-rmse:1.03938\n[1400]\tvalidation_0-rmse:1.04085\n[1500]\tvalidation_0-rmse:1.04214\n[1600]\tvalidation_0-rmse:1.04255\n[1666]\tvalidation_0-rmse:1.04276\n\nOriginal Model Pearson scores by slice:\n  full_data: 0.1167\n  last_75pct: 0.1007\n  last_50pct: 0.0987\n\nOriginal Model Ensemble Pearson score: 0.1123\n\nSaved original submission.csv\n\n================================================================================\nMODEL 2: AUGMENTED MODEL\n================================================================================\n\nPreparing augmented features...\n\nCreating targeted augmented features...\n  Created 20 augmented features:\n    - complex_interaction_862_852_345\n    - poly_ratio_862_852\n    - X862_minus_X852\n    - X852_minus_X345\n    - rbf_862_852\n    - kyle_lambda_complex\n    - vol_adjusted_pressure\n    - trade_intensity_asymmetry\n    - bid_minus_ask\n    - volume_gaussian_kernel\n    - volume_weighted_technical\n    - X532_plus_X888\n    - rbf_532_888\n    - regime_indicator_137_302\n    - laplacian_kernel_137_302\n    - multidim_distance_kernel\n    - X178_minus_X168\n    - X168_minus_X612\n    - X598_minus_X862\n    - rbf_598_862\n\nCreating targeted augmented features...\n  Created 20 augmented features:\n    - complex_interaction_862_852_345\n    - poly_ratio_862_852\n    - X862_minus_X852\n    - X852_minus_X345\n    - rbf_862_852\n    - kyle_lambda_complex\n    - vol_adjusted_pressure\n    - trade_intensity_asymmetry\n    - bid_minus_ask\n    - volume_gaussian_kernel\n    - volume_weighted_technical\n    - X532_plus_X888\n    - rbf_532_888\n    - regime_indicator_137_302\n    - laplacian_kernel_137_302\n    - multidim_distance_kernel\n    - X178_minus_X168\n    - X168_minus_X612\n    - X598_minus_X862\n    - rbf_598_862\n\nTotal features in augmented model: 48\nNew features added: 20\n\nTraining Augmented Model...\n\n--- Augmented Model Fold 1/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.00216\n[100]\tvalidation_0-rmse:0.99630\n[200]\tvalidation_0-rmse:0.99445\n[300]\tvalidation_0-rmse:0.99311\n[400]\tvalidation_0-rmse:0.99261\n[500]\tvalidation_0-rmse:0.99330\n[600]\tvalidation_0-rmse:0.99390\n[700]\tvalidation_0-rmse:0.99511\n[800]\tvalidation_0-rmse:0.99586\n[900]\tvalidation_0-rmse:0.99699\n[1000]\tvalidation_0-rmse:0.99845\n[1100]\tvalidation_0-rmse:0.99969\n[1200]\tvalidation_0-rmse:1.00098\n[1300]\tvalidation_0-rmse:1.00248\n[1400]\tvalidation_0-rmse:1.00361\n[1500]\tvalidation_0-rmse:1.00542\n[1600]\tvalidation_0-rmse:1.00623\n[1666]\tvalidation_0-rmse:1.00726\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.00216\n[100]\tvalidation_0-rmse:0.99640\n[200]\tvalidation_0-rmse:0.99326\n[300]\tvalidation_0-rmse:0.99246\n[400]\tvalidation_0-rmse:0.99281\n[500]\tvalidation_0-rmse:0.99238\n[600]\tvalidation_0-rmse:0.99291\n[700]\tvalidation_0-rmse:0.99376\n[800]\tvalidation_0-rmse:0.99506\n[900]\tvalidation_0-rmse:0.99606\n[1000]\tvalidation_0-rmse:0.99724\n[1100]\tvalidation_0-rmse:0.99815\n[1200]\tvalidation_0-rmse:0.99962\n[1300]\tvalidation_0-rmse:1.00065\n[1400]\tvalidation_0-rmse:1.00175\n[1500]\tvalidation_0-rmse:1.00398\n[1600]\tvalidation_0-rmse:1.00531\n[1666]\tvalidation_0-rmse:1.00629\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.00306\n[100]\tvalidation_0-rmse:0.99593\n[200]\tvalidation_0-rmse:0.99271\n[300]\tvalidation_0-rmse:0.99115\n[400]\tvalidation_0-rmse:0.99096\n[500]\tvalidation_0-rmse:0.99134\n[600]\tvalidation_0-rmse:0.99266\n[700]\tvalidation_0-rmse:0.99356\n[800]\tvalidation_0-rmse:0.99516\n[900]\tvalidation_0-rmse:0.99643\n[1000]\tvalidation_0-rmse:0.99802\n[1100]\tvalidation_0-rmse:0.99946\n[1200]\tvalidation_0-rmse:1.00170\n[1300]\tvalidation_0-rmse:1.00330\n[1400]\tvalidation_0-rmse:1.00472\n[1500]\tvalidation_0-rmse:1.00672\n[1600]\tvalidation_0-rmse:1.00780\n[1666]\tvalidation_0-rmse:1.00915\n\n--- Augmented Model Fold 2/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.00748\n[100]\tvalidation_0-rmse:1.00478\n[200]\tvalidation_0-rmse:1.00508\n[300]\tvalidation_0-rmse:1.00481\n[400]\tvalidation_0-rmse:1.00540\n[500]\tvalidation_0-rmse:1.00642\n[600]\tvalidation_0-rmse:1.00708\n[700]\tvalidation_0-rmse:1.00797\n[800]\tvalidation_0-rmse:1.00939\n[900]\tvalidation_0-rmse:1.01031\n[1000]\tvalidation_0-rmse:1.01111\n[1100]\tvalidation_0-rmse:1.01259\n[1200]\tvalidation_0-rmse:1.01401\n[1300]\tvalidation_0-rmse:1.01533\n[1400]\tvalidation_0-rmse:1.01652\n[1500]\tvalidation_0-rmse:1.01745\n[1600]\tvalidation_0-rmse:1.01859\n[1666]\tvalidation_0-rmse:1.01977\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.00818\n[100]\tvalidation_0-rmse:1.00672\n[200]\tvalidation_0-rmse:1.00822\n[300]\tvalidation_0-rmse:1.01011\n[400]\tvalidation_0-rmse:1.01145\n[500]\tvalidation_0-rmse:1.01503\n[600]\tvalidation_0-rmse:1.01791\n[700]\tvalidation_0-rmse:1.02047\n[800]\tvalidation_0-rmse:1.02416\n[900]\tvalidation_0-rmse:1.02773\n[1000]\tvalidation_0-rmse:1.03118\n[1100]\tvalidation_0-rmse:1.03424\n[1200]\tvalidation_0-rmse:1.03685\n[1300]\tvalidation_0-rmse:1.03953\n[1400]\tvalidation_0-rmse:1.04233\n[1500]\tvalidation_0-rmse:1.04559\n[1600]\tvalidation_0-rmse:1.04801\n[1666]\tvalidation_0-rmse:1.04914\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.00850\n[100]\tvalidation_0-rmse:1.00616\n[200]\tvalidation_0-rmse:1.00750\n[300]\tvalidation_0-rmse:1.00867\n[400]\tvalidation_0-rmse:1.01004\n[500]\tvalidation_0-rmse:1.01162\n[600]\tvalidation_0-rmse:1.01352\n[700]\tvalidation_0-rmse:1.01540\n[800]\tvalidation_0-rmse:1.01822\n[900]\tvalidation_0-rmse:1.02020\n[1000]\tvalidation_0-rmse:1.02273\n[1100]\tvalidation_0-rmse:1.02508\n[1200]\tvalidation_0-rmse:1.02753\n[1300]\tvalidation_0-rmse:1.02881\n[1400]\tvalidation_0-rmse:1.03029\n[1500]\tvalidation_0-rmse:1.03167\n[1600]\tvalidation_0-rmse:1.03379\n[1666]\tvalidation_0-rmse:1.03493\n\n--- Augmented Model Fold 3/3 ---\nTraining full_data...\n[0]\tvalidation_0-rmse:1.02089\n[100]\tvalidation_0-rmse:1.01749\n[200]\tvalidation_0-rmse:1.02021\n[300]\tvalidation_0-rmse:1.02491\n[400]\tvalidation_0-rmse:1.02858\n[500]\tvalidation_0-rmse:1.03155\n[600]\tvalidation_0-rmse:1.03612\n[700]\tvalidation_0-rmse:1.04015\n[800]\tvalidation_0-rmse:1.04293\n[900]\tvalidation_0-rmse:1.04603\n[1000]\tvalidation_0-rmse:1.04836\n[1100]\tvalidation_0-rmse:1.05154\n[1200]\tvalidation_0-rmse:1.05257\n[1300]\tvalidation_0-rmse:1.05450\n[1400]\tvalidation_0-rmse:1.05730\n[1500]\tvalidation_0-rmse:1.05907\n[1600]\tvalidation_0-rmse:1.06120\n[1666]\tvalidation_0-rmse:1.06153\nTraining last_75pct...\n[0]\tvalidation_0-rmse:1.02074\n[100]\tvalidation_0-rmse:1.01654\n[200]\tvalidation_0-rmse:1.01860\n[300]\tvalidation_0-rmse:1.02030\n[400]\tvalidation_0-rmse:1.02504\n[500]\tvalidation_0-rmse:1.02840\n[600]\tvalidation_0-rmse:1.03245\n[700]\tvalidation_0-rmse:1.03605\n[800]\tvalidation_0-rmse:1.03881\n[900]\tvalidation_0-rmse:1.04143\n[1000]\tvalidation_0-rmse:1.04360\n[1100]\tvalidation_0-rmse:1.04586\n[1200]\tvalidation_0-rmse:1.04924\n[1300]\tvalidation_0-rmse:1.05042\n[1400]\tvalidation_0-rmse:1.05245\n[1500]\tvalidation_0-rmse:1.05449\n[1600]\tvalidation_0-rmse:1.05650\n[1666]\tvalidation_0-rmse:1.05799\nTraining last_50pct...\n[0]\tvalidation_0-rmse:1.01951\n[100]\tvalidation_0-rmse:1.01993\n[200]\tvalidation_0-rmse:1.02400\n[300]\tvalidation_0-rmse:1.02580\n[400]\tvalidation_0-rmse:1.02850\n[500]\tvalidation_0-rmse:1.03318\n[600]\tvalidation_0-rmse:1.03600\n[700]\tvalidation_0-rmse:1.03908\n[800]\tvalidation_0-rmse:1.04169\n[900]\tvalidation_0-rmse:1.04428\n[1000]\tvalidation_0-rmse:1.04665\n[1100]\tvalidation_0-rmse:1.04857\n[1200]\tvalidation_0-rmse:1.04996\n[1300]\tvalidation_0-rmse:1.05241\n[1400]\tvalidation_0-rmse:1.05331\n[1500]\tvalidation_0-rmse:1.05417\n[1600]\tvalidation_0-rmse:1.05473\n[1666]\tvalidation_0-rmse:1.05535\n\nAugmented Model Pearson scores by slice:\n  full_data: 0.1071\n  last_75pct: 0.0888\n  last_50pct: 0.0866\n\nAugmented Model Ensemble Pearson score: 0.1009\n\n================================================================================\nFINAL ENSEMBLE\n================================================================================\n\nSaved ensemble submission_ensemble.csv\n\n================================================================================\nRESULTS SUMMARY\n================================================================================\nOriginal Model:\n  Features: 28\n  Score: 0.1123\n\nAugmented Model:\n  Features: 48\n  Score: 0.1009\n\nExpected Ensemble Performance:\n  Better than: 0.1009\n  Potential: ~0.1066 to 0.1146\n\n================================================================================\nAUGMENTED FEATURES IMPORTANCE CHECK\n================================================================================\n\nImportance of new augmented features:\n  complex_interaction_862_852_345: Rank 16/48, Importance: 0.0260\n  poly_ratio_862_852: Rank 29/48, Importance: 0.0215\n  X862_minus_X852: Rank 1/48, Importance: 0.0372\n  X852_minus_X345: Rank 10/48, Importance: 0.0279\n  rbf_862_852: Rank 12/48, Importance: 0.0268\n  kyle_lambda_complex: Rank 48/48, Importance: 0.0033\n  vol_adjusted_pressure: Rank 41/48, Importance: 0.0084\n  trade_intensity_asymmetry: Rank 43/48, Importance: 0.0066\n  bid_minus_ask: Rank 47/48, Importance: 0.0036\n  volume_gaussian_kernel: Rank 45/48, Importance: 0.0041\n  volume_weighted_technical: Rank 19/48, Importance: 0.0251\n  X532_plus_X888: Rank 15/48, Importance: 0.0263\n  rbf_532_888: Rank 33/48, Importance: 0.0187\n  regime_indicator_137_302: Rank 6/48, Importance: 0.0292\n  laplacian_kernel_137_302: Rank 30/48, Importance: 0.0211\n  multidim_distance_kernel: Rank 13/48, Importance: 0.0268\n  X178_minus_X168: Rank 23/48, Importance: 0.0233\n  X168_minus_X612: Rank 14/48, Importance: 0.0264\n  X598_minus_X862: Rank 2/48, Importance: 0.0318\n  rbf_598_862: Rank 39/48, Importance: 0.0147\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom scipy.stats import pearsonr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:28:32.462115Z","iopub.execute_input":"2025-06-24T07:28:32.462911Z","iopub.status.idle":"2025-06-24T07:28:37.780562Z","shell.execute_reply.started":"2025-06-24T07:28:32.462873Z","shell.execute_reply":"2025-06-24T07:28:37.779661Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def feature_engineering(df):\n\n    df['volume_weighted_sell'] = df['sell_qty'] * df['volume']\n    df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-8)\n    df['selling_pressure'] = df['sell_qty'] / (df['volume'] + 1e-8)\n    df['effective_spread_proxy'] = np.abs(df['buy_qty'] - df['sell_qty']) / (df['volume'] + 1e-8)\n\n\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    return df ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:28:48.338674Z","iopub.execute_input":"2025-06-24T07:28:48.339485Z","iopub.status.idle":"2025-06-24T07:28:48.345748Z","shell.execute_reply.started":"2025-06-24T07:28:48.339447Z","shell.execute_reply":"2025-06-24T07:28:48.344777Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Config:\n    TRAIN_PATH = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n    TEST_PATH = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n    SUBMISSION_PATH = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n\n    FEATURES = [\n        \"X863\", \"X856\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n        \"X415\", \"X345\", \"X855\", \"X174\", \"X302\", \"X178\", \"X168\", \"X612\",\n        \"buy_qty\", \"sell_qty\", \"volume\", \"X888\", \"X421\", \"X333\"\n    ]\n\n    LABEL_COLUMN = \"label\"\n    N_FOLDS = 3\n    RANDOM_STATE = 42\n\nXGB_PARAMS = {\n    \"tree_method\": \"hist\",\n    \"device\": \"gpu\",\n    \"colsample_bylevel\": 0.4778,\n    \"colsample_bynode\": 0.3628,\n    \"colsample_bytree\": 0.7107,\n    \"gamma\": 1.7095,\n    \"learning_rate\": 0.02213,\n    \"max_depth\": 20,\n    \"max_leaves\": 12,\n    \"min_child_weight\": 16,\n    \"n_estimators\": 1667,\n    \"subsample\": 0.06567,\n    \"reg_alpha\": 39.3524,\n    \"reg_lambda\": 75.4484,\n    \"verbosity\": 0,\n    \"random_state\": Config.RANDOM_STATE,\n    \"n_jobs\": -1\n}\n\nLEARNERS = [\n    {\"name\": \"xgb\", \"Estimator\": XGBRegressor, \"params\": XGB_PARAMS},\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:28:59.043897Z","iopub.execute_input":"2025-06-24T07:28:59.044265Z","iopub.status.idle":"2025-06-24T07:28:59.052422Z","shell.execute_reply.started":"2025-06-24T07:28:59.044240Z","shell.execute_reply":"2025-06-24T07:28:59.051239Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_time_decay_weights(n: int, decay: float = 0.9) -> np.ndarray:\n    positions = np.arange(n)\n    normalized = positions / (n - 1)\n    weights = decay ** (1.0 - normalized)\n    return weights * n / weights.sum()\n\ndef load_data():\n    train_df = pd.read_parquet(Config.TRAIN_PATH, columns=Config.FEATURES + [Config.LABEL_COLUMN])\n    test_df = pd.read_parquet(Config.TEST_PATH, columns=Config.FEATURES)\n    submission_df = pd.read_csv(Config.SUBMISSION_PATH)\n\n    train_df = feature_engineering(train_df)\n    test_df = feature_engineering(test_df)\n    print(f\"Loaded data - Train: {train_df.shape}, Test: {test_df.shape}, Submission: {submission_df.shape}\")\n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True), submission_df\nConfig.FEATURES += [\"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"]\nConfig.FEATURES = list(set(Config.FEATURES))  # remove duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:29:09.876080Z","iopub.execute_input":"2025-06-24T07:29:09.876452Z","iopub.status.idle":"2025-06-24T07:29:09.883976Z","shell.execute_reply.started":"2025-06-24T07:29:09.876424Z","shell.execute_reply":"2025-06-24T07:29:09.882602Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_model_slices(n_samples: int):\n    return [\n        {\"name\": \"full_data\", \"cutoff\": 0},\n        {\"name\": \"last_75pct\", \"cutoff\": int(0.25 * n_samples)},\n        {\"name\": \"last_50pct\", \"cutoff\": int(0.50 * n_samples)},\n    ]\n\ndef train_and_evaluate(train_df, test_df):\n    n_samples = len(train_df)\n    model_slices = get_model_slices(n_samples)\n\n    oof_preds = {\n        learner[\"name\"]: {s[\"name\"]: np.zeros(n_samples) for s in model_slices}\n        for learner in LEARNERS\n    }\n    test_preds = {\n        learner[\"name\"]: {s[\"name\"]: np.zeros(len(test_df)) for s in model_slices}\n        for learner in LEARNERS\n    }\n\n    full_weights = create_time_decay_weights(n_samples)\n    kf = KFold(n_splits=Config.N_FOLDS, shuffle=False)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df), start=1):\n        print(f\"\\n--- Fold {fold}/{Config.N_FOLDS} ---\")\n        X_valid = train_df.iloc[valid_idx][Config.FEATURES]\n        y_valid = train_df.iloc[valid_idx][Config.LABEL_COLUMN]\n\n        for s in model_slices:\n            cutoff = s[\"cutoff\"]\n            slice_name = s[\"name\"]\n            subset = train_df.iloc[cutoff:].reset_index(drop=True)\n            rel_idx = train_idx[train_idx >= cutoff] - cutoff\n\n            X_train = subset.iloc[rel_idx][Config.FEATURES]\n            y_train = subset.iloc[rel_idx][Config.LABEL_COLUMN]\n            sw = create_time_decay_weights(len(subset))[rel_idx] if cutoff > 0 else full_weights[train_idx]\n\n            print(f\"  Training slice: {slice_name}, samples: {len(X_train)}\")\n\n            for learner in LEARNERS:\n                model = learner[\"Estimator\"](**learner[\"params\"])\n                model.fit(X_train, y_train, sample_weight=sw, eval_set=[(X_valid, y_valid)], verbose=False)\n\n                mask = valid_idx >= cutoff\n                if mask.any():\n                    idxs = valid_idx[mask]\n                    oof_preds[learner[\"name\"]][slice_name][idxs] = model.predict(train_df.iloc[idxs][Config.FEATURES])\n                if cutoff > 0 and (~mask).any():\n                    oof_preds[learner[\"name\"]][slice_name][valid_idx[~mask]] = oof_preds[learner[\"name\"]][\"full_data\"][valid_idx[~mask]]\n\n                test_preds[learner[\"name\"]][slice_name] += model.predict(test_df[Config.FEATURES])\n\n    # Normalize test predictions\n    for learner_name in test_preds:\n        for slice_name in test_preds[learner_name]:\n            test_preds[learner_name][slice_name] /= Config.N_FOLDS\n\n    return oof_preds, test_preds, model_slices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:29:23.333243Z","iopub.execute_input":"2025-06-24T07:29:23.333644Z","iopub.status.idle":"2025-06-24T07:29:23.346605Z","shell.execute_reply.started":"2025-06-24T07:29:23.333616Z","shell.execute_reply":"2025-06-24T07:29:23.345682Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def ensemble_and_submit(train_df, oof_preds, test_preds, submission_df):\n    learner_ensembles = {}\n    for learner_name in oof_preds:\n        scores = {s: pearsonr(train_df[Config.LABEL_COLUMN], oof_preds[learner_name][s])[0]\n                  for s in oof_preds[learner_name]}\n        total_score = sum(scores.values())\n\n        oof_simple = np.mean(list(oof_preds[learner_name].values()), axis=0)\n        test_simple = np.mean(list(test_preds[learner_name].values()), axis=0)\n        score_simple = pearsonr(train_df[Config.LABEL_COLUMN], oof_simple)[0]\n\n        oof_weighted = sum(scores[s] / total_score * oof_preds[learner_name][s] for s in scores)\n        test_weighted = sum(scores[s] / total_score * test_preds[learner_name][s] for s in scores)\n        score_weighted = pearsonr(train_df[Config.LABEL_COLUMN], oof_weighted)[0]\n\n        print(f\"\\n{learner_name.upper()} Simple Ensemble Pearson:   {score_simple:.4f}\")\n        print(f\"{learner_name.upper()} Weighted Ensemble Pearson: {score_weighted:.4f}\")\n\n        learner_ensembles[learner_name] = {\n            \"oof_simple\": oof_simple,\n            \"test_simple\": test_simple\n        }\n\n    final_oof = np.mean([le[\"oof_simple\"] for le in learner_ensembles.values()], axis=0)\n    final_test = np.mean([le[\"test_simple\"] for le in learner_ensembles.values()], axis=0)\n    final_score = pearsonr(train_df[Config.LABEL_COLUMN], final_oof)[0]\n\n    print(f\"\\nFINAL ensemble across learners Pearson: {final_score:.4f}\")\n\n    submission_df[\"prediction\"] = final_test\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"Saved: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:29:35.073070Z","iopub.execute_input":"2025-06-24T07:29:35.073522Z","iopub.status.idle":"2025-06-24T07:29:35.089321Z","shell.execute_reply.started":"2025-06-24T07:29:35.073477Z","shell.execute_reply":"2025-06-24T07:29:35.088375Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_df, test_df, submission_df = load_data()\n    oof_preds, test_preds, model_slices = train_and_evaluate(train_df, test_df)\n    ensemble_and_submit(train_df, oof_preds, test_preds, submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:29:47.528172Z","iopub.execute_input":"2025-06-24T07:29:47.528488Z","iopub.status.idle":"2025-06-24T07:43:33.438448Z","shell.execute_reply.started":"2025-06-24T07:29:47.528467Z","shell.execute_reply":"2025-06-24T07:43:33.437113Z"}},"outputs":[{"name":"stdout","text":"Loaded data - Train: (525887, 30), Test: (538150, 29), Submission: (538150, 2)\n\n--- Fold 1/3 ---\n  Training slice: full_data, samples: 350591\n  Training slice: last_75pct, samples: 350591\n  Training slice: last_50pct, samples: 262944\n\n--- Fold 2/3 ---\n  Training slice: full_data, samples: 350591\n  Training slice: last_75pct, samples: 219120\n  Training slice: last_50pct, samples: 175295\n\n--- Fold 3/3 ---\n  Training slice: full_data, samples: 350592\n  Training slice: last_75pct, samples: 219121\n  Training slice: last_50pct, samples: 87649\n\nXGB Simple Ensemble Pearson:   0.1094\nXGB Weighted Ensemble Pearson: 0.1097\n\nFINAL ensemble across learners Pearson: 0.1094\nSaved: submission.csv\n","output_type":"stream"}],"execution_count":9}]}